{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4glFUe2R9uRc"
   },
   "source": [
    "# Music 103 diffusion version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-iydqiNU5fGu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "from os import remove, chdir\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-J1xVnVE_6Zm"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_len = x.size(1)\n",
    "        pe = torch.zeros(max_len, self.d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * -(math.log(10000.0) / self.d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).to(x.device)\n",
    "        return x + pe\n",
    "\n",
    "\n",
    "class DecoderPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, tgt):\n",
    "        # max_len = x.size(1)\n",
    "        tgt_one_hot = tgt[:, :, 12:]\n",
    "        tgt_class = torch.argmax(tgt_one_hot, dim=-1)\n",
    "        pe = torch.zeros_like(x)\n",
    "        position = torch.cumsum(tgt_class, dim=1).unsqueeze(-1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * -(math.log(10000.0) / self.d_model)).to(position.device)\n",
    "        \n",
    "        pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        return x + pe\n",
    "\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class EmbedHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        inner_dim_1,\n",
    "        inner_dim_2,\n",
    "        out_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, inner_dim_1)\n",
    "        self.linear2 = nn.Linear(inner_dim_1, inner_dim_2)\n",
    "        self.linear3 = nn.Linear(inner_dim_2, out_dim)\n",
    "        self.activation_fn = nn.functional.gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = EmbedHead(src_vocab_size, d_model, d_model, d_model)\n",
    "        self.decoder_embedding = EmbedHead(tgt_vocab_size, d_model, d_model, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (torch.sum(src, dim=2) > 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (torch.sum(tgt, dim=2) > 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        \n",
    "        d = 1\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=d)).bool().to(src.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        if src_mask is None or tgt_mask is None:\n",
    "            src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for i, dec_layer in enumerate(self.decoder_layers):\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, dropout, codebook_size, d_codebook):\n",
    "        super().__init__()\n",
    "        self.codebook_size = codebook_size\n",
    "        self.encoder_embedding = EmbedHead(vocab_size, d_model, d_model, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.encoder_output = nn.Linear(d_model, d_codebook)\n",
    "        self.codebook = nn.Embedding(codebook_size, d_codebook)\n",
    "        self.codebook.weight.data.uniform_(-1/d_codebook, 1/d_codebook)\n",
    "        self.decoder_embedding = EmbedHead(d_codebook, d_model, d_model, d_model)\n",
    "        self.decoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_output = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def encode(self, x):\n",
    "        embedding = self.dropout(self.positional_encoding(self.encoder_embedding(x)))\n",
    "        for i, enc_layer in enumerate(self.encoder_layers):\n",
    "            embedding = enc_layer(embedding, None)\n",
    "        return self.encoder_output(embedding)\n",
    "    \n",
    "    def vq_indices(self, z):\n",
    "        distance = (z.unsqueeze(2) - self.codebook.weight.unsqueeze(0).unsqueeze(0)).pow(2).mean(dim=-1)\n",
    "        _, indices = torch.min(distance, dim=-1)\n",
    "        return indices\n",
    "    \n",
    "    def vq_one_hot(self, z):\n",
    "        indices = self.vq_indices(z)\n",
    "        one_hot = torch.nn.functional.one_hot(indices, num_classes=self.codebook_size).float()\n",
    "        return one_hot\n",
    "\n",
    "    def vq(self, z):\n",
    "        return self.codebook(self.vq_indices(z))\n",
    "\n",
    "    def decode(self, z):\n",
    "        embedding = self.dropout(self.positional_encoding(self.decoder_embedding(z)))\n",
    "        for i, dec_layer in enumerate(self.decoder_layers):\n",
    "            embedding = dec_layer(embedding, None)\n",
    "        return torch.sigmoid(self.decoder_output(embedding))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length, vocab_size]\n",
    "        z = self.encode(x)\n",
    "        z_vq = self.vq(z)\n",
    "        z_straight_through = (z_vq - z).detach() + z\n",
    "        x_recon = self.decode(z_straight_through)\n",
    "        recon_loss = nn.functional.binary_cross_entropy(x_recon, x)\n",
    "        embed_loss = nn.functional.mse_loss(z_vq, z.detach())\n",
    "        commit_loss = nn.functional.mse_loss(z, z_vq.detach())\n",
    "        return x_recon, recon_loss, embed_loss, commit_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_5PFYwtohYY"
   },
   "source": [
    "# **1. DDPM**\n",
    "\n",
    "\n",
    "# a. Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yAENstIAuDh"
   },
   "source": [
    "# b. DDPM Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeL8cLF-A9u-"
   },
   "source": [
    "# c. DDPM Main Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the noise $\\sigma_t^2=\\beta_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYUg7AjHottB"
   },
   "source": [
    "# c. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hrG8n5WJAo51"
   },
   "outputs": [],
   "source": [
    "def train_main_loop(transformer, vqvae, optim, trainset, validset, lr, n_epoch, device, patience):\n",
    "    wait = 0\n",
    "    min_valid_loss = float('inf')\n",
    "    for ep in tqdm(range(n_epoch)):\n",
    "        transformer.train()\n",
    "\n",
    "        # linear lrate decay\n",
    "        optim.param_groups[0]['lr'] = lr*(1-ep/n_epoch)\n",
    "        # train\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        for idx, src, tgt in trainset:\n",
    "            optim.zero_grad()\n",
    "            tgt = tgt.to(device)\n",
    "            src = src.to(device)\n",
    "            src_mask, tgt_mask = transformer.generate_mask(src, tgt[:, :-1, :])\n",
    "            tgt_indices = vqvae.vq_indices(vqvae.encode(tgt))\n",
    "            tgt_one_hot = torch.nn.functional.one_hot(tgt_indices, num_classes=vqvae.codebook_size).float()\n",
    "            output = transformer(src, tgt_one_hot[:, :-1, :])\n",
    "            loss = criterion(output.contiguous().view(-1, vqvae.codebook_size), tgt_indices[:, 1:].contiguous().view(-1))\n",
    "            loss_train = loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        # validation\n",
    "        transformer.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, src, tgt in validset:\n",
    "                tgt = tgt.to(device)\n",
    "                src = src.to(device)\n",
    "                src_mask, tgt_mask = transformer.generate_mask(src, tgt[:, :-1, :])\n",
    "                tgt_indices = vqvae.vq_indices(vqvae.encode(tgt))\n",
    "                tgt_one_hot = torch.nn.functional.one_hot(tgt_indices, num_classes=vqvae.codebook_size).float()\n",
    "                output = transformer(src, tgt_one_hot[:, :-1, :], src_mask, tgt_mask)\n",
    "                loss = criterion(output.contiguous().view(-1, vqvae.codebook_size), tgt_indices[:, 1:].contiguous().view(-1))\n",
    "                total_loss += loss.item()\n",
    "        avg_valid_loss = total_loss / len(validset)\n",
    "\n",
    "        # early stopping\n",
    "        if avg_valid_loss < min_valid_loss:\n",
    "            min_valid_loss = avg_valid_loss\n",
    "            torch.save(transformer.state_dict(), f\"model_best_autoreg.pt\")\n",
    "            print(f'epoch {ep}, train_loss: {loss_train:.4f}, valid loss: {avg_valid_loss:.4f}')\n",
    "            wait = 0\n",
    "        else:\n",
    "            print(f'epoch {ep}, train_loss: {loss_train:.4f}, valid loss: {avg_valid_loss:.4f}, min_valid_loss: {min_valid_loss:.4f}, wait: {wait} / {patience}')\n",
    "            wait += 1\n",
    "        if wait >= patience:\n",
    "            break\n",
    "\n",
    "def eval_main_loop(transformer, vqvae, checkpoint, testset, device, guide_w, rate=0.5):\n",
    "    transformer.load_state_dict(torch.load(checkpoint))\n",
    "    transformer.eval()\n",
    "    x_gens = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, src, tgt in tqdm(testset, total=len(testset)):\n",
    "            if count > 10:\n",
    "                break\n",
    "            \n",
    "            tgt_enc = vqvae.vq_one_hot(vqvae.encode(tgt))\n",
    "            sampled_indices = []\n",
    "            current_tgt_enc = tgt_enc[:, :1, :]\n",
    "            for t in range(1, tgt_enc.size(1)):\n",
    "                output = transformer(src, current_tgt_enc).detach()\n",
    "                tgt_enc_prediction = torch.softmax(output[:, -1, :], dim=-1)\n",
    "                # sample from categorical distribution\n",
    "                sampled_index = torch.multinomial(tgt_enc_prediction, 1)\n",
    "                sampled_indices.append(sampled_index)\n",
    "                sampled_one_hot = torch.nn.functional.one_hot(sampled_index, num_classes=vqvae.codebook_size).float()\n",
    "                current_tgt_enc = torch.cat([current_tgt_enc, sampled_one_hot], dim=1)\n",
    "            x_gen = vqvae.decode(vqvae.codebook(torch.cat(sampled_indices, dim=1)))\n",
    "            x_gen = (x_gen >= rate).long()\n",
    "            x_gens.append((idx, x_gen))\n",
    "            count += 1\n",
    "\n",
    "    torch.save(x_gens, \"song_test_music103.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Il39DdgeBPKc"
   },
   "source": [
    "# e. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted dataset found!\n"
     ]
    }
   ],
   "source": [
    "# hardcoding these here\n",
    "n_epoch = 200\n",
    "n_T = 1000\n",
    "n_feat = 128\n",
    "lr = 1e-4\n",
    "ws_test = [0.0, 0.5, 2.0]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "src_vocab_size = 12\n",
    "tgt_vocab_size = 128\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "d_ff = 4096//8\n",
    "max_seq_length = 2400\n",
    "dropout = 0.1\n",
    "batchsize = 16\n",
    "mode = \"train\"\n",
    "\n",
    "\n",
    "if exists(\"trainset_w.pkl\") and exists(\"validset_w.pkl\") and exists(\"testset_w.pkl\"):\n",
    "    print(\"splitted dataset found!\")\n",
    "    with open(\"trainset_w.pkl\", \"rb\") as f:\n",
    "        trainset = pickle.load(f)\n",
    "    with open(\"validset_w.pkl\", \"rb\") as f:\n",
    "        validset = pickle.load(f)\n",
    "    with open(\"testset_w.pkl\", \"rb\") as f:\n",
    "        testset = pickle.load(f)\n",
    "else:\n",
    "    print(\"?\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unpack batch into individual components\n",
    "    idx, src_data, tgt_data, w = zip(*batch)\n",
    "    #print(len(rates[0]), len(tgt_data[0]), len(src_data[0]))\n",
    "    \n",
    "    # Convert `src_data`, `tgt_data`, and `rates` to tensors if they are not already\n",
    "    src_data = [torch.tensor(s, dtype=torch.float32) if not isinstance(s, torch.Tensor) else s for s in src_data]\n",
    "    tgt_data = [torch.tensor(t, dtype=torch.float32) if not isinstance(t, torch.Tensor) else t for t in tgt_data]\n",
    "\n",
    "    tgt_data = [torch.cat([torch.zeros(1, 12), t], dim=0) for t in tgt_data]\n",
    "\n",
    "    # Pad src_data\n",
    "    src_data = nn.utils.rnn.pad_sequence(src_data, batch_first=True, padding_value=0.).to(DEVICE)\n",
    "\n",
    "    # Pad tgt_data\n",
    "    tgt_data = nn.utils.rnn.pad_sequence(tgt_data, batch_first=True, padding_value=0).to(DEVICE)\n",
    "\n",
    "    # Extract the last dimension and one-hot encode it\n",
    "    return idx, src_data, tgt_data\n",
    "\n",
    "\n",
    "trainset = data.DataLoader(trainset, batch_size=batchsize, collate_fn=collate_fn)\n",
    "validset = data.DataLoader(validset, batch_size=1, collate_fn=collate_fn)\n",
    "testset = data.DataLoader(testset, batch_size=1, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L1-Pnrj8BTqr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:08<29:11,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss: 2.7433, valid loss: 3.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:17<28:46,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss: 2.5741, valid loss: 3.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:25<28:17,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, train_loss: 2.2084, valid loss: 2.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:34<28:00,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, train_loss: 1.8246, valid loss: 2.6139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:43<28:00,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, train_loss: 1.6812, valid loss: 2.5380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:51<27:44,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, train_loss: 1.5437, valid loss: 2.3626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [01:00<27:32,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, train_loss: 1.3831, valid loss: 2.1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [01:08<27:30,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, train_loss: 1.2340, valid loss: 1.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [01:17<27:20,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, train_loss: 1.0965, valid loss: 1.8918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [01:26<27:10,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, train_loss: 1.0914, valid loss: 1.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [01:34<27:03,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, train_loss: 1.0263, valid loss: 1.7840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [01:43<26:54,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, train_loss: 1.0329, valid loss: 1.7537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 13/200 [01:51<26:46,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, train_loss: 1.0117, valid loss: 1.7240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [02:00<26:39,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, train_loss: 0.9887, valid loss: 1.7049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/200 [02:09<26:31,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, train_loss: 0.9485, valid loss: 1.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [02:17<26:26,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, train_loss: 0.9634, valid loss: 1.6775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [02:26<26:18,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, train_loss: 0.9576, valid loss: 1.6661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [02:34<26:11,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, train_loss: 0.9456, valid loss: 1.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [02:43<26:02,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, train_loss: 0.9341, valid loss: 1.6496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [02:52<25:53,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, train_loss: 0.9351, valid loss: 1.6385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [03:00<25:41,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, train_loss: 0.9209, valid loss: 1.6392, min_valid_loss: 1.6385, wait: 0 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 22/200 [03:09<25:34,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, train_loss: 0.9149, valid loss: 1.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [03:18<25:27,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22, train_loss: 0.9004, valid loss: 1.6213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 24/200 [03:26<25:20,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23, train_loss: 0.8916, valid loss: 1.6172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 25/200 [03:35<25:13,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24, train_loss: 0.8806, valid loss: 1.6112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 26/200 [03:43<24:59,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25, train_loss: 0.8858, valid loss: 1.6121, min_valid_loss: 1.6112, wait: 0 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 27/200 [03:52<24:53,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26, train_loss: 0.8750, valid loss: 1.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [04:01<24:40,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, train_loss: 0.8691, valid loss: 1.5998, min_valid_loss: 1.5995, wait: 0 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 29/200 [04:09<24:29,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28, train_loss: 0.8571, valid loss: 1.5997, min_valid_loss: 1.5995, wait: 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [04:18<24:24,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29, train_loss: 0.8459, valid loss: 1.5962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [04:27<24:16,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30, train_loss: 0.8578, valid loss: 1.5909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 32/200 [04:35<24:10,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31, train_loss: 0.8261, valid loss: 1.5904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 33/200 [04:44<23:57,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32, train_loss: 0.8005, valid loss: 1.5955, min_valid_loss: 1.5904, wait: 0 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/200 [04:52<23:46,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33, train_loss: 0.8209, valid loss: 1.6047, min_valid_loss: 1.5904, wait: 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 35/200 [05:01<23:36,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, train_loss: 0.8192, valid loss: 1.6046, min_valid_loss: 1.5904, wait: 2 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 36/200 [05:09<23:26,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35, train_loss: 0.8192, valid loss: 1.5925, min_valid_loss: 1.5904, wait: 3 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 37/200 [05:18<23:21,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36, train_loss: 0.8235, valid loss: 1.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 38/200 [05:27<23:10,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37, train_loss: 0.8168, valid loss: 1.6000, min_valid_loss: 1.5869, wait: 0 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 39/200 [05:35<23:00,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38, train_loss: 0.8006, valid loss: 1.6185, min_valid_loss: 1.5869, wait: 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [05:44<22:51,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39, train_loss: 0.7942, valid loss: 1.6259, min_valid_loss: 1.5869, wait: 2 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [05:52<22:43,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, train_loss: 0.7988, valid loss: 1.6140, min_valid_loss: 1.5869, wait: 3 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 42/200 [06:01<22:34,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41, train_loss: 0.7832, valid loss: 1.6147, min_valid_loss: 1.5869, wait: 4 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 43/200 [06:09<22:25,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42, train_loss: 0.7985, valid loss: 1.6306, min_valid_loss: 1.5869, wait: 5 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 44/200 [06:18<22:16,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43, train_loss: 0.7705, valid loss: 1.6282, min_valid_loss: 1.5869, wait: 6 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 45/200 [06:27<22:07,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44, train_loss: 0.7760, valid loss: 1.6192, min_valid_loss: 1.5869, wait: 7 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 46/200 [06:35<22:00,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45, train_loss: 0.7570, valid loss: 1.6376, min_valid_loss: 1.5869, wait: 8 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 47/200 [06:44<21:51,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46, train_loss: 0.7523, valid loss: 1.6511, min_valid_loss: 1.5869, wait: 9 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 48/200 [06:52<21:43,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47, train_loss: 0.7562, valid loss: 1.6582, min_valid_loss: 1.5869, wait: 10 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/200 [07:01<21:34,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48, train_loss: 0.7478, valid loss: 1.6467, min_valid_loss: 1.5869, wait: 11 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [07:10<21:32,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, train_loss: 0.7477, valid loss: 1.6485, min_valid_loss: 1.5869, wait: 12 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 51/200 [07:18<21:20,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, train_loss: 0.7217, valid loss: 1.6360, min_valid_loss: 1.5869, wait: 13 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 52/200 [07:27<21:10,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51, train_loss: 0.7084, valid loss: 1.6399, min_valid_loss: 1.5869, wait: 14 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 53/200 [07:35<21:00,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52, train_loss: 0.7216, valid loss: 1.6456, min_valid_loss: 1.5869, wait: 15 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 54/200 [07:44<20:51,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53, train_loss: 0.7226, valid loss: 1.6604, min_valid_loss: 1.5869, wait: 16 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 55/200 [07:52<20:41,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, train_loss: 0.7140, valid loss: 1.6735, min_valid_loss: 1.5869, wait: 17 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 56/200 [08:01<20:32,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55, train_loss: 0.7185, valid loss: 1.6504, min_valid_loss: 1.5869, wait: 18 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 56/200 [08:10<21:00,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56, train_loss: 0.7161, valid loss: 1.6682, min_valid_loss: 1.5869, wait: 19 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, 3, d_ff, max_seq_length, dropout).to(DEVICE)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=lr)\n",
    "vqvae = VQVAE(12, 512, num_heads, 1, d_ff, dropout, 128, 12).to(DEVICE)\n",
    "vqvae.load_state_dict(torch.load(\"model_best_vqvae_128.pt\"))\n",
    "train_main_loop(transformer, vqvae, optim, trainset, validset, lr, n_epoch, DEVICE, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:29<03:57,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_main_loop(transformer, vqvae,\"model_best_autoreg.pt\", testset, DEVICE, 2, 0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deepml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
