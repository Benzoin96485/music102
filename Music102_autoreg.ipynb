{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4glFUe2R9uRc"
   },
   "source": [
    "# Music 103 diffusion version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-iydqiNU5fGu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "from os import remove, chdir\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-J1xVnVE_6Zm"
   },
   "outputs": [],
   "source": [
    "with open(\"D12_Q.pkl\", \"rb\") as f:\n",
    "    D12_Q = pickle.load(f)\n",
    "for k in [\"E1\", \"E2\", \"E3\", \"E4\", \"E5\"]:\n",
    "    D12_Q[k] *= math.sqrt(2)\n",
    "\n",
    "\n",
    "class D12_featurize(nn.Module):\n",
    "    def __init__(self, D12_Q, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.weight = nn.Parameter(torch.ones(1))\n",
    "        self.Q = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.tensor(v, dtype=dtype), requires_grad=False) for k, v in D12_Q.items()\n",
    "        })\n",
    "    \n",
    "    def forward(self, D12_vec_perm):\n",
    "        return {\n",
    "            k: v @ (D12_vec_perm + self.bias).unsqueeze(-1) * self.weight for k, v in self.Q.items()\n",
    "        }\n",
    "\n",
    "# class D12_linear(nn.Module):\n",
    "#     def __init__(self, input_mult, output_mult):\n",
    "#         super().__init__()\n",
    "#         self.din = sum(input_mult.values())\n",
    "#         self.dout = sum(output_mult.values())\n",
    "#         self.linear = nn.Linear(self.din, self.dout)\n",
    "#         self.Q = nn.ParameterDict({\n",
    "#             k: nn.Parameter(torch.tensor(v, dtype=torch.float32), requires_grad=False) for k, v in D12_Q.items()\n",
    "#         })\n",
    "#         self.keys = [\"A1\", \"B2\", \"E1\", \"E2\", \"E3\", \"E4\", \"E5\"]\n",
    "#         self.splits = [output_mult[k] for k in self.keys]\n",
    "\n",
    "#     def forward(self, D12_vec):\n",
    "#         out = dict()\n",
    "#         D12_perm = torch.concat([self.Q[k].T @ D12_vec[k] for k in self.keys], dim=-1)\n",
    "#         D12_perm = self.linear(D12_perm)\n",
    "#         D12_perm_split = torch.split(D12_perm, self.splits, dim=-1)\n",
    "#         for i, k in enumerate(self.keys):\n",
    "#             out[k] = self.Q[k] @ D12_perm_split[i]\n",
    "#         return out\n",
    "\n",
    "class D12_linear_single(nn.Module):\n",
    "    def __init__(self, input_mult, output_mult):\n",
    "        super().__init__()\n",
    "        self.input_mult = input_mult\n",
    "        self.output_mult = output_mult\n",
    "        self.linear_channels = nn.ModuleDict({\n",
    "            k: nn.Linear(v, output_mult[k], bias=False) for k, v in input_mult.items()\n",
    "        })\n",
    "        self.Q = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.tensor(v, dtype=torch.float32), requires_grad=False) for k, v in D12_Q.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, D12_vec):\n",
    "        return {\n",
    "            k: self.linear_channels[k](v) for k, v in D12_vec.items()\n",
    "        }\n",
    "\n",
    "D12_linear = D12_linear_single\n",
    "\n",
    "def D12_L2(D12_vec):\n",
    "    return {k: torch.sqrt(torch.sum(v ** 2, dim=-2, keepdim=True)) for k, v in D12_vec.items()}\n",
    "\n",
    "\n",
    "class Activation(nn.Module):\n",
    "    def __init__(self, mult, activation_fn, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        if activation_fn == \"gelu\":\n",
    "            self.activation_fn = nn.functional.gelu\n",
    "        elif activation_fn == \"relu\":\n",
    "            self.activation_fn = nn.functional.relu\n",
    "        self.Q = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.tensor(v, dtype=dtype), requires_grad=False) for k, v in D12_Q.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, D12_vec):\n",
    "        D12_vec_perm = {k: self.activation_fn(self.Q[k].T @ v) for k, v in D12_vec.items()}\n",
    "        return {k: self.Q[k] @ v for k, v in D12_vec_perm.items()}\n",
    "\n",
    "\n",
    "class LayerNorm_invariant(nn.Module):\n",
    "    def __init__(self, mult):\n",
    "        super().__init__()\n",
    "        self.Q = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.tensor(v, dtype=torch.float32), requires_grad=False) for k, v in D12_Q.items()\n",
    "        })\n",
    "        self.gamma = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.ones(v, dtype=torch.float32), requires_grad=True) for k, v in mult.items()\n",
    "        })\n",
    "        self.beta = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.zeros(v, dtype=torch.float32), requires_grad=True) for k, v in mult.items()\n",
    "        })\n",
    "\n",
    "    def normalize(self, v, eps=1e-5):\n",
    "        # var, mean = torch.var_mean(v, dim=-1, keepdim=True)\n",
    "        var, mean = torch.var_mean(v, dim=(-2,-1), keepdim=True)\n",
    "        return (v - mean) / torch.sqrt(var + eps)\n",
    "\n",
    "    def forward(self, D12_vec):\n",
    "        return {k: self.Q[k] @ (self.normalize(self.Q[k].T @ v) * self.gamma[k] + self.beta[k]) for k, v in D12_vec.items()}\n",
    "\n",
    "\n",
    "class D12_FC_out(nn.Module):\n",
    "    def __init__(self, mult, D12_Q, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.linear3 = D12_linear(mult, {k: 1 for k in mult})\n",
    "        self.Q = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.tensor(v, dtype=dtype), requires_grad=False) for k, v in D12_Q.items()\n",
    "        })\n",
    "        self.keys = [\"A1\", \"B2\", \"E1\", \"E2\", \"E3\", \"E4\", \"E5\"]\n",
    "    \n",
    "    def forward(self, D12_vec):\n",
    "        D12_vec = self.linear3(D12_vec)\n",
    "        D12_perm = torch.concat([self.Q[k].T @ D12_vec[k] for k in self.keys], dim=-1)\n",
    "        return torch.mean(D12_perm, dim=-1)\n",
    "\n",
    "\n",
    "class EmbedHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_mult,\n",
    "        inner_mult_1,\n",
    "        inner_mult_2,\n",
    "        out_mult,\n",
    "        D12_Q\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            D12_featurize(D12_Q),\n",
    "            D12_linear(input_mult, inner_mult_1),\n",
    "            Activation(inner_mult_1, \"gelu\"),\n",
    "            D12_linear(inner_mult_1, inner_mult_2),\n",
    "            Activation(inner_mult_2, \"gelu\"),\n",
    "            D12_linear(inner_mult_2, out_mult)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, mult_model, num_heads):\n",
    "        super().__init__()\n",
    "        for v in mult_model.values():\n",
    "            assert v % num_heads == 0, \"multiplicity must be divisible by num_heads\"\n",
    "        \n",
    "        self.mult_model = mult_model\n",
    "        self.num_heads = num_heads\n",
    "        self.mult_k = {k: v // num_heads for k, v in mult_model.items()}\n",
    "        self.keys = [\"A1\", \"B2\", \"E1\", \"E2\", \"E3\", \"E4\", \"E5\"]\n",
    "        self.W_q = D12_linear(self.mult_model, self.mult_model)\n",
    "        self.W_k = D12_linear(self.mult_model, self.mult_model)\n",
    "        self.W_v = D12_linear(self.mult_model, self.mult_model)\n",
    "        self.W_o = D12_linear(self.mult_model, self.mult_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        Q = torch.cat([Q[k] for k in self.keys], dim=-1) # batch_size, self.num_heads, seq_length, sum(dim_rep * self.mult_k)\n",
    "        K = torch.cat([K[k] for k in self.keys], dim=-1) \n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.shape[-1])\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = {k: torch.matmul(attn_probs, v) for k, v in V.items()}\n",
    "        return output\n",
    "         \n",
    "    def split_heads(self, D12_vec):\n",
    "        heads = dict()\n",
    "        for k, v in D12_vec.items():\n",
    "            batch_size, seq_length, dim_rep, mult = v.size()\n",
    "            heads[k] = (v\n",
    "                .reshape(batch_size, seq_length, dim_rep, self.num_heads, self.mult_k[k])\n",
    "                .permute(0, 3, 1, 2, 4) # batch_size, self.num_heads, seq_length, dim_rep, self.mult_k\n",
    "                .reshape(batch_size, self.num_heads, seq_length, -1) # batch_size, self.num_heads, seq_length, dim_rep * self.mult_k\n",
    "            )\n",
    "        return heads\n",
    "        \n",
    "    def combine_heads(self, heads):\n",
    "        D12_vec = dict()\n",
    "        for k, v in heads.items():\n",
    "            batch_size, _, seq_length, self.mult_k_rep = v.size()\n",
    "            D12_vec[k] = (v\n",
    "                .reshape(batch_size, self.num_heads, seq_length, -1, self.mult_k[k])\n",
    "                # batch_size, self.num_heads, seq_length, dim_rep, self.mult_k\n",
    "                .permute(0, 2, 3, 1, 4) # batch_size, seq_length, dim_rep, self.num_heads, self.mult_k\n",
    "                .reshape(batch_size, seq_length, -1, self.mult_model[k])\n",
    "            )\n",
    "        return D12_vec\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, mult_model, mult_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = D12_linear(mult_model, mult_ff)\n",
    "        self.fc2 = D12_linear(mult_ff, mult_model)\n",
    "        self.relu = Activation(mult_ff, \"gelu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, mult_model, num_heads, mult_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(mult_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(mult_model, mult_ff)\n",
    "        self.norm1 = lambda x: x\n",
    "        self.norm2 = lambda x: x\n",
    "        # self.norm1 = LayerNorm_invariant(mult_model)\n",
    "        # self.norm2 = LayerNorm_invariant(mult_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1({k: x[k] + attn_output[k] for k in x})\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2({k: x[k] + ff_output[k] for k in x})\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, mult_model, num_heads, mult_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(mult_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(mult_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(mult_model, mult_ff)\n",
    "        # self.norm1 = LayerNorm_invariant(mult_model)\n",
    "        # self.norm2 = LayerNorm_invariant(mult_model)\n",
    "        # self.norm3 = LayerNorm_invariant(mult_model)\n",
    "        self.norm1 = lambda x: x\n",
    "        self.norm2 = lambda x: x\n",
    "        self.norm3 = lambda x: x\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1({k: x[k] + self.dropout(attn_output[k]) for k in x})\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2({k: x[k] + self.dropout(attn_output[k]) for k in x})\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3({k: x[k] + self.dropout(ff_output[k]) for k in x})\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, mult_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.mult_model = mult_model\n",
    "        self.ones = nn.Parameter(torch.ones(12,1), requires_grad=False)\n",
    "        self.Q = nn.ParameterDict({\n",
    "            k: nn.Parameter(torch.tensor(v, dtype=torch.float32), requires_grad=False) for k, v in D12_Q.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded_x = dict()\n",
    "        for k, v in x.items():\n",
    "            max_len = v.size(1)\n",
    "            pe = torch.zeros(max_len, 1, self.mult_model[k])\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, self.mult_model[k], 2).float() * -(math.log(10000.0) / self.mult_model[k]))\n",
    "            pe[:, :, 0::2] = torch.sin(position * div_term).unsqueeze(1)\n",
    "            pe[:, :, 1::2] = torch.cos(position * div_term).unsqueeze(1)\n",
    "            pe = pe.unsqueeze(0).to(v.device)\n",
    "            encoded_x[k] = v + self.Q[k] @ (self.ones @ pe)\n",
    "        return encoded_x\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, mult_input, mult_model, num_heads, num_layers, mult_ff, dropout, D12_Q):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = EmbedHead(mult_input, mult_model, mult_model, mult_model, D12_Q)\n",
    "        self.decoder_embedding = EmbedHead(mult_input, mult_model, mult_model, mult_model, D12_Q)\n",
    "        self.positional_encoding = PositionalEncoding(mult_model)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(mult_model, num_heads, mult_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(mult_model, num_heads, mult_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = D12_FC_out(mult_model, D12_Q)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (torch.sum(src, dim=2) > 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (torch.sum(tgt, dim=2) > 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        \n",
    "        d = 1\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=d)).bool().to(src.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.positional_encoding(self.encoder_embedding(src))\n",
    "        tgt_embedded = self.positional_encoding(self.decoder_embedding(tgt))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for i, dec_layer in enumerate(self.decoder_layers):\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        output = self.fc(dec_output)\n",
    "        logit = torch.sigmoid(output)\n",
    "        return logit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYUg7AjHottB"
   },
   "source": [
    "# c. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hrG8n5WJAo51"
   },
   "outputs": [],
   "source": [
    "def train_main_loop(transformer, optim, trainset, validset, lr, n_epoch, patience):\n",
    "    wait = 0\n",
    "    min_valid_loss = float('inf')\n",
    "    for ep in tqdm(range(n_epoch)):\n",
    "        transformer.train()\n",
    "\n",
    "        # linear lrate decay\n",
    "        optim.param_groups[0]['lr'] = lr*(1-ep/n_epoch)\n",
    "        # train\n",
    "        criterion = nn.BCELoss()\n",
    "        for idx, src, tgt in trainset:\n",
    "            optim.zero_grad()\n",
    "            output = transformer(src, tgt[:, :-1, :])\n",
    "            loss = criterion(output.contiguous().view(-1), tgt[:, 1:, :].contiguous().view(-1))\n",
    "            loss_train = loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        # validation\n",
    "        transformer.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, src, tgt in validset:\n",
    "                output = transformer(src, tgt[:, :-1, :])\n",
    "                loss = criterion(output.contiguous().view(-1), tgt[:, 1:, :].contiguous().view(-1))\n",
    "                total_loss += loss.item()\n",
    "        avg_valid_loss = total_loss / len(validset)\n",
    "\n",
    "        # early stopping\n",
    "        if avg_valid_loss < min_valid_loss:\n",
    "            min_valid_loss = avg_valid_loss\n",
    "            torch.save(transformer.state_dict(), f\"model_best_autoreg_equi.pt\")\n",
    "            print(f'epoch {ep}, train_loss: {loss_train:.4f}, valid loss: {avg_valid_loss:.4f}')\n",
    "            wait = 0\n",
    "        else:\n",
    "            print(f'epoch {ep}, train_loss: {loss_train:.4f}, valid loss: {avg_valid_loss:.4f}, min_valid_loss: {min_valid_loss:.4f}, wait: {wait} / {patience}')\n",
    "            wait += 1\n",
    "        if wait >= patience:\n",
    "            break\n",
    "\n",
    "def eval_main_loop(transformer, checkpoint, testset, rate=0.5):\n",
    "    transformer.load_state_dict(torch.load(checkpoint))\n",
    "    transformer.eval()\n",
    "    x_gens = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, src, tgt in tqdm(testset, total=len(testset)):\n",
    "            if count > 10:\n",
    "                break\n",
    "            \n",
    "            current_tgt = tgt[:, :1, :]\n",
    "            for t in range(1, tgt.size(1)):\n",
    "                output = transformer(src, current_tgt).detach()\n",
    "                # sample from categorical distribution\n",
    "                sampled_tgt = torch.bernoulli(output[:, -1, :])\n",
    "                current_tgt = torch.cat([current_tgt, sampled_tgt], dim=1)\n",
    "            x_gen = (current_tgt >= rate).long()\n",
    "            x_gens.append((idx, x_gen))\n",
    "            count += 1\n",
    "\n",
    "    torch.save(x_gens, \"song_test_music103.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Il39DdgeBPKc"
   },
   "source": [
    "# e. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted dataset found!\n"
     ]
    }
   ],
   "source": [
    "# hardcoding these here\n",
    "n_epoch = 200\n",
    "n_T = 1000\n",
    "n_feat = 128\n",
    "lr = 1e-4\n",
    "ws_test = [0.0, 0.5, 2.0]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "src_vocab_size = 12\n",
    "tgt_vocab_size = 128\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "d_ff = 4096//8\n",
    "max_seq_length = 2400\n",
    "dropout = 0.1\n",
    "batchsize = 16\n",
    "mode = \"train\"\n",
    "\n",
    "\n",
    "if exists(\"trainset_w.pkl\") and exists(\"validset_w.pkl\") and exists(\"testset_w.pkl\"):\n",
    "    print(\"splitted dataset found!\")\n",
    "    with open(\"trainset_w.pkl\", \"rb\") as f:\n",
    "        trainset = pickle.load(f)\n",
    "    with open(\"validset_w.pkl\", \"rb\") as f:\n",
    "        validset = pickle.load(f)\n",
    "    with open(\"testset_w.pkl\", \"rb\") as f:\n",
    "        testset = pickle.load(f)\n",
    "else:\n",
    "    print(\"?\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unpack batch into individual components\n",
    "    idx, src_data, tgt_data, w = zip(*batch)\n",
    "    #print(len(rates[0]), len(tgt_data[0]), len(src_data[0]))\n",
    "    \n",
    "    # Convert `src_data`, `tgt_data`, and `rates` to tensors if they are not already\n",
    "    src_data = [torch.tensor(s, dtype=torch.float32) if not isinstance(s, torch.Tensor) else s for s in src_data]\n",
    "    tgt_data = [torch.tensor(t, dtype=torch.float32) if not isinstance(t, torch.Tensor) else t for t in tgt_data]\n",
    "\n",
    "    tgt_data = [torch.cat([torch.zeros(1, 12), t], dim=0) for t in tgt_data]\n",
    "\n",
    "    # Pad src_data\n",
    "    src_data = nn.utils.rnn.pad_sequence(src_data, batch_first=True, padding_value=0.).to(DEVICE)\n",
    "\n",
    "    # Pad tgt_data\n",
    "    tgt_data = nn.utils.rnn.pad_sequence(tgt_data, batch_first=True, padding_value=0).to(DEVICE)\n",
    "\n",
    "    # Extract the last dimension and one-hot encode it\n",
    "    return idx, src_data, tgt_data\n",
    "\n",
    "\n",
    "trainset = data.DataLoader(trainset, batch_size=batchsize, collate_fn=collate_fn)\n",
    "validset = data.DataLoader(validset, batch_size=1, collate_fn=collate_fn)\n",
    "testset = data.DataLoader(testset, batch_size=1, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "L1-Pnrj8BTqr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [01:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m transformer \u001b[38;5;241m=\u001b[39m Transformer(input_mult, model_mult, num_heads, num_layers, model_mult, dropout, D12_Q)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      5\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(transformer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mtrain_main_loop\u001b[0;34m(transformer, optim, trainset, validset, lr, n_epoch, patience)\u001b[0m\n\u001b[1;32m     12\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer(src, tgt[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/python-LLM-2023b/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/python-LLM-2023b/lib/python3.10/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/python-LLM-2023b/lib/python3.10/site-packages/torch/nn/functional.py:3098\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3096\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "input_mult = {k: 1 for k in D12_Q}\n",
    "model_mult = {k: 64 for k in D12_Q}\n",
    "transformer = Transformer(input_mult, model_mult, num_heads, num_layers, model_mult, dropout, D12_Q).to(DEVICE)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=lr)\n",
    "\n",
    "train_main_loop(transformer, optim, trainset, validset, lr, n_epoch, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 11/100 [00:28<03:46,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_main_loop(transformer, vqvae,\"model_best_autoreg.pt\", testset, DEVICE, 2, 0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deepml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
